---
output: github_document
always_allow_html: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.align = "center",
  message = FALSE,
  warning = FALSE
)

library(kableExtra)
```

# MachineShop: Machine Learning Models and Tools

[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/MachineShop)](http://cran.r-project.org/web/packages/MachineShop)

## Overview

`MachineShop` is a meta-package for statistical and machine learning with a common interface for model fitting, prediction, performance assessment, and presentation of results.  Support is provided for predictive modeling of numerical, categorical, and censored time-to-event outcomes, including those listed in the table below, and for resample (bootstrap, cross-validation, and split training-test sets) estimation of model performance.

<div>
```{r echo = FALSE}
library(MachineShop)

info <- modelinfo()
types <- c("binary" = "b", "factor" = "f", "matrix" = "m", "numeric" = "n",
           "ordered" = "o", "Surv" = "S")
x <- lapply(names(info), function(modelname) {
  c(modelname, ifelse(names(types) %in% info[[modelname]]$types, types, NA))
})
df <- as.data.frame(do.call(rbind, x), stringsAsFactors = FALSE)
names(df) <- c("Constructor", names(types))
rownames(df) <- sapply(info, getElement, name = "label")

toString2 <- function(x) toString(na.omit(x))
df_classes <- data.frame(
  Constructor = df$Constructor,
  Categorical = apply(df[c("binary", "factor", "ordered")], 1, toString2),
  Continuous = apply(df[c("matrix", "numeric")], 1, toString2),
  Survival = apply(df["Surv"], 1, toString2)
)
names(df_classes)[-1] <- paste0(names(df_classes)[-1],
                                footnote_marker_number(1:3))

kable(df_classes, align = "c", escape = FALSE) %>%
  kable_styling("striped", full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, " " = 1, "Response Variable Types" = 3)) %>%
  footnote(number = c("b = binary, f = factor, o = ordered",
                      "m = matrix, n = numeric",
                      "S = Surv"))
```
</div>

## Installation

```{r eval = FALSE}
# Current release from CRAN
install.packages("MachineShop")

# Development version from GitHub
# install.packages("devtools")
devtools::install_github("brian-j-smith/MachineShop", ref = "develop")

# Development version with vignettes
devtools::install_github("brian-j-smith/MachineShop", ref = "develop", build_vignettes = TRUE)
```


## Documentation

Once the package is installed, general documentation on its usage can be viewed with the following console commands.

```{r eval = FALSE, message = FALSE}
library(MachineShop)

# Package help summary
?MachineShop

# Vignette
RShowDoc("Introduction", package = "MachineShop")
```


## Parallel Computing

Resampling algorithms will be executed in parallel automatically if a parallel backend for the ``foreach`` package, such as ``doParallel``, is loaded.

```{r}
library(doParallel)
registerDoParallel(cores = 4)
```


## Example

The following is a brief example illustrating use of the package to predict the species of flowers in Edgar Anderson's iris data set.

### Training and Test Set Analysis

```{r}
## Load the package
library(MachineShop)
library(magrittr)

## Iris flower species (3 level response) data set
head(iris)

## Training and test sets
set.seed(123)
trainindices <- sample(nrow(iris), nrow(iris) * 2 / 3)
train <- iris[trainindices, ]
test <- iris[-trainindices, ]

## Model formula
fo <- Species ~ .

## Gradient boosted mode fit to training set
gbmfit <- fit(fo, data = train, model = GBMModel)

## Variable importance
(vi <- varimp(gbmfit))

plot(vi)
```

```{r}
## Test set predicted probabilities
predict(gbmfit, newdata = test, type = "prob") %>% head

## Test set predicted classifications
predict(gbmfit, newdata = test) %>% head

## Test set performance
obs <- response(fo, data = test)
pred <- predict(gbmfit, newdata = test, type = "prob")
modelmetrics(obs, pred)
```

### Resampling

```{r, fig.height = 4}
## Resample estimation of model performance
(perf <- resample(fo, data = iris, model = GBMModel, control = CVControl))

summary(perf)

plot(perf)
```

### Model Tuning

```{r, fig.height = 4}
## Tune over a grid of model parameters
gbmtune <- tune(fo, data = iris, model = GBMModel,
                grid = expand.grid(n.trees = c(25, 50, 100),
                                   interaction.depth = 1:3,
                                   n.minobsinnode = c(5, 10)))

plot(gbmtune, type = "line")
```

```{r}
## Fit the selected model
gbmtunefit <- fit(fo, data = iris, model = gbmtune)
varimp(gbmtunefit)
```

### Model Comparisons

```{r}
## Model comparisons
control <- CVControl(folds = 10, repeats = 5)

gbmperf <- resample(fo, data = iris, model = GBMModel(n.tree = 50), control = control)
rfperf <- resample(fo, data = iris, model = RandomForestModel(ntree = 50), control = control)
nnetperf <- resample(fo, data = iris, model = NNetModel(size = 5), control = control)

perf <- Resamples(GBM = gbmperf, RF = rfperf, NNet = nnetperf)
summary(perf)

plot(perf)
```

```{r}
## Pairwise model differences and t-tests
perfdiff <- diff(perf)
summary(perfdiff)

t.test(perfdiff)

plot(perfdiff)
```

### Ensemble Models

```{r}
## Stacked regression
stackedperf <- resample(fo, data = iris, model = StackedModel(GBMModel, RandomForestModel, NNetModel))
summary(stackedperf)

## Super learners
superperf <- resample(fo, data = iris, model = SuperModel(GBMModel, RandomForestModel, NNetModel))
summary(superperf)
```

### Calibration Curves

```{r results = "hide"}
cal <- calibration(perf)
plot(cal, se = TRUE)
```

### Confusion Matrices

```{r}
(conf <- confusion(gbmperf))

summary(conf)
```

```{r results = "hide"}
plot(conf)
```

### Partial Dependence Plots

```{r results = "hide"}
pd <- dependence(gbmfit, select = c(Petal.Length, Petal.Width))
plot(pd)
```

### Lift Curves

```{r}
## Requires a binary outcome
fo_versicolor <- factor(Species == "versicolor") ~ .
control = CVControl()

gbmperf_versicolor <- resample(fo_versicolor, data = iris,  model = GBMModel, control = control)
lf <- lift(gbmperf_versicolor)
plot(lf)
```

```{r}
rfperf_versicolor <- resample(fo_versicolor, data = iris,  model = RandomForestModel, control = control)
nnetperf_versicolor <- resample(fo_versicolor, data = iris,  model = NNetModel, control = control)

perf_versicolor <- Resamples(gbmperf_versicolor, rfperf_versicolor, nnetperf_versicolor)
lf <- lift(perf_versicolor)
plot(lf, find = 75)
```

### Preprocessing Recipes

```{r}
library(recipes)

rec <- recipe(fo, data = iris) %>%
  add_role(Species, new_role = "case_strata") %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors())

fit_rec <- fit(rec, model = GBMModel)
varimp(fit_rec)

perf_rec <- resample(rec, model = GBMModel, control = CVControl)
summary(perf_rec)
```
