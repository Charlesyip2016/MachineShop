# Modeling Strategies


## Model Tuning

Many of the modeling functions have arguments, or tuning parameters, that control aspects of their model fitting algorithms.  For example, `GBMModel` parameters `n.trees` and `interaction.depth` control the number of decision trees to fit and the maximum depth of variable interactions.  The `tune` function performs model fitting over a grid of parameter values and returns the model with the most optimal values.  Optimality is determined based on the first performance metric of the `metrics` argument of `tune` if supplied or the first default metric of the `performance` function otherwise.  Argument `grid` additionally controls the construction of grid values and can be a single numeric value giving the grid length in each parameter dimension.  As shown in the output below, tuning results include the tuning parameter grid values, the names of models fit to each, all calculated metrics, the final model selected, the metric upon which its selection was based, and its tuning parameters.

```{r using_strategies_tune}
## Tune over automatic grid of model parameters
(tuned_model <- tune(surv_fo, data = surv_train, model = GBMModel,
                     grid = 3,
                     control = surv_means_control,
                     metrics = c("CIndex" = cindex, "RMSE" = rmse)))
```

Grid values may also be a call to `Grid` with the grid `length` and number of grid points to sample at `random` or a user-specified data frame of grid points.

```{r using_strategies_tune_grid, eval=FALSE}
## Tune over randomly sampled grid points
tune(surv_fo, data = surv_train, model = GBMModel,
     grid = Grid(length = 100, random = 10),
     control = surv_means_control)

## Tune over user-specified grid points
tune(surv_fo, data = surv_train, model = GBMModel,
     grid = expand_params(n.trees = c(25, 50, 100),
                          interaction.depth = 1:3),
     control = surv_means_control)
```

Statistics summarizing the resampled performance metrics across all tuning parameter combinations can be obtained with the `summary` function.
  
```{r using_strategies_tune_summary}
summary(tuned_model)
```

Line plots of tuning results display the resampled metric means, or another statistic specified with the `stat` argument, versus the first tuning parameter values and with lines grouped according to the remaining parameters, if any.

```{r using_strategies_tune_plot}
plot(tuned_model, type = "line")
```

The return value of `tune` is a model object with the optimal tuning parameters and not a model fit object.  The returned model can be fit subsequently to a set of data with the `fit` function.  Having the final model allows for fitting to the same dataset on which tuning was performed or to datasets from other case populations.

```{r using_strategies_tune_fit}
## Fit the tuned model
surv_fit <- fit(surv_fo, data = surv_train, model = tuned_model)
```

As illustrated in the examples above, the `tune` function allows for comparisons of all models fit to the parameter grid points and returns the optimal model for subsequent fitting.  For applications in which fitting, performance assessment, and prediction with the optimal model are of primary interest, the `TunedModel` function is provided as a model interface to the `tune` function.  The model interface has the advantage of enabling predictive performance estimation, with the `resample` function, that accounts for the full tuning grid process.

```{r using_strategies_tune_model}
## Model interface for grid tuning
tuned_model <- TunedModel(GBMModel, grid = 3, control = surv_means_control,
                          metrics = c("CIndex" = cindex, "RMSE" = rmse))
surv_fit <- fit(surv_fo, data = surv_train, model = tuned_model)
(vi <- varimp(surv_fit))

## Predictive performance of the tuning process
res_tuned <- resample(surv_fo, data = surv_train, model = tuned_model,
                      control = surv_means_control)
summary(res_tuned)
```


## Model Selection

Model selection can be conducted with the `tune` or `SelectedModel` function to automatically choose from any combination of models and model parameters.  Selection has as a special case the just-discussed tuning of a single model over a grid of parameter values.  Combinations of model functions, function names, or function calls can be supplied as lists to the `models` argument of `tune` or as arguments to `SelectedModel` in order to define sets of candidate models from which to select.  An `expand_model` helper function is additionally available to expand a model over a grid of tuning parameters for inclusion in the candidate set if so desired.  In this general form of model selection, the `grid` argument in the tuning functions is not used.

```{r using_strategies_selection, results="hide"}
## Model interface for model selection
selected_model <- SelectedModel(
  expand_model(GBMModel, n.trees = c(50, 100), interaction.depth = 1:2),
  GLMNetModel(lambda = 0.01),
  CoxModel,
  SurvRegModel
)

## Fit the selected model
fit(surv_fo, data = surv_train, model = selected_model)
```

Selection may also be performed over candidate sets that include tuned models.  For instance, the `SelectedModel` function is applicable to sets containing different classes of models each individually tuned over a grid of parameters.

```{r using_strategies_selection_tuned, results="hide"}
## Model interface for selection among tuned models
selected_tuned_model <- SelectedModel(
  TunedModel(GBMModel, control = surv_means_control),
  TunedModel(GLMNetModel, control = surv_means_control),
  TunedModel(CoxModel, control = surv_means_control)
)

## Fit the selected tuned model
fit(surv_fo, data = surv_train, model = selected_tuned_model)
```


## Ensemble Models

Ensemble modelling methods combine $m = 1, \ldots, M$ base learning models as a strategy to improve predictive performance.  Two methods implemented in `Machineshop` are *stacked regression* [@breiman:1996:SR] and *super learners* [@vanderLann:2007:SL].  Stacked regression fits a linear combination of predictions from specified base learners to produce a prediction function of the form
$$
\hat{f}(x) = \sum_{m=1}^M \hat{w}_m \hat{f}_m(x).
$$
Stacking weights $w$ are estimated by (constrained) least squares regression of case responses $y_i$ on predictions $\hat{f}^{-\kappa(i)}(x_i)$ from learners fit to data subsamples $-\kappa(i)$ not containing the corresponding cases. In particular, they are obtained as the solution
$$
\hat{w} = \underset{w}{\operatorname{argmin}} \sum_{i=1}^{N}\left(y_i - \sum_{m=1}^{M} w_m \hat{f}^{-\kappa(i)}(x_i) \right)^2
$$
subject to the constraints that all $w_m \ge 0$ and $\sum_m w_m = 1$.  K-fold cross-validation is the default subsampling method employed in the estimation, with the other resampling methods provided by the package available as other options.  Survival outcomes are handled with a modified version of the stacked regression algorithm in which

* minimization of least squares is replaced by maximization of Harrell's concordance index [-@harrell:1982:EYM] to accommodate censoring, and
* prediction can only be performed on the same response type used for the model fit; i.e., either survival means or survival probabilities at set follow-up times.

Super learners are a generalization of stacked regression that fit a specified model, such as `GBMModel`, to case responses $y_i$, base learner predictions $\hat{f}^{-\kappa(i)}(x_i)$, and optionally also to the original predictor variables $x_i$.  Given below are examples of a stacked regression and super learner each fit with gradient boosted, random forest, and Cox regression base learners.  A separate gradient boosted model is used as the super learner in the latter.

```{r using_strategies_ensembles}
## Stacked regression
stackedmodel <- StackedModel(GLMBoostModel, CForestModel, CoxModel)
res_stacked <- resample(surv_fo, data = surv_train, model = stackedmodel)
summary(res_stacked)

## Super learner
supermodel <- SuperModel(GLMBoostModel, CForestModel, CoxModel,
                         model = GBMModel)
res_super <- resample(surv_fo, data = surv_train, model = supermodel)
summary(res_super)
```
