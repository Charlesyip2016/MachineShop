# Modeling Strategies


## Model Tuning

Many of the modeling functions have arguments, or tuning parameters, that control aspects of their model fitting algorithms.  For example, `GBMModel` parameters `n.trees` and `interaction.depth` control the number of decision trees to fit and the maximum depth of variable interactions.  When called with a `TunedModel`, the `fit` function performs model fitting over a grid of parameter values and returns the model with the most optimal values.  Optimality is determined based on the first performance metric of the `metrics` argument of `TunedModel` if supplied or the first default metric of the `performance` function otherwise.  Argument `grid` additionally controls the construction of grid values and can be a single numeric value giving the grid length in each parameter dimension.  As shown in the output below, `as.MLModel` will extract a tuned model from fit results for viewing of the tuning parameter grid values, the names of models fit to each, all calculated metrics, the final model selected, the metric upon which its selection was based, and its tuning parameters.

```{r using_strategies_tune}
## Tune over automatic grid of model parameters
surv_fit <- TunedModel(
  GBMModel,
  grid = 3,
  control = surv_means_control,
  metrics = c("CIndex" = cindex, "RMSE" = rmse)
) %>% fit(surv_fo, data = surv_train)
(tuned_model <- as.MLModel(surv_fit))
```

Grid values may also be a call to `Grid` with the grid `length` and number of grid points to sample at `random` or a user-specified data frame of grid points.

```{r using_strategies_tune_grid, eval=FALSE}
## Tune over randomly sampled grid points
TunedModel(
  GBMModel,
  grid = Grid(length = 100, random = 10),
  control = surv_means_control
) %>% fit(surv_fo, data = surv_train)

## Tune over user-specified grid points
TunedModel(
  GBMModel,
  grid = expand_params(n.trees = c(25, 50, 100),
                       interaction.depth = 1:3),
  control = surv_means_control
) %>% fit(surv_fo, data = surv_train)
```

Statistics summarizing the resampled performance metrics across all tuning parameter combinations can be obtained with the `summary` function.
  
```{r using_strategies_tune_summary}
summary(tuned_model)
```

Line plots of tuning results display the resampled metric means, or another statistic specified with the `stat` argument, versus the first tuning parameter values and with lines grouped according to the remaining parameters, if any.

```{r using_strategies_tune_plot}
plot(tuned_model, type = "line")
```


## Model Selection

Model selection can be conducted by calling `fit` with a `SelectedModel` to automatically choose from any combination of models and model parameters.  Selection has as a special case the just-discussed tuning of a single model over a grid of parameter values.  Combinations of model functions, function names, or function calls can be supplied as lists to the `models` argument of `SelectedModel` in order to define sets of candidate models from which to select.  An `expand_model` helper function is additionally available to expand a model over a grid of tuning parameters for inclusion in the candidate set if so desired.

```{r using_strategies_select, results="hide"}
## Model interface for model selection
selected_model <- SelectedModel(
  expand_model(GBMModel, n.trees = c(50, 100), interaction.depth = 1:2),
  GLMNetModel(lambda = 0.01),
  CoxModel,
  SurvRegModel
)

## Fit the selected model
fit(surv_fo, data = surv_train, model = selected_model)
```

Selection may also be performed over candidate sets that include tuned models.  For instance, the `SelectedModel` function is applicable to sets containing different classes of models each individually tuned over a grid of parameters.

```{r using_strategies_select_tune, results="hide"}
## Model interface for selection among tuned models
selected_tuned_model <- SelectedModel(
  TunedModel(GBMModel, control = surv_means_control),
  TunedModel(GLMNetModel, control = surv_means_control),
  TunedModel(CoxModel, control = surv_means_control)
)

## Fit the selected tuned model
fit(surv_fo, data = surv_train, model = selected_tuned_model)
```


## Ensemble Models

Ensemble modelling methods combine $m = 1, \ldots, M$ base learning models as a strategy to improve predictive performance.  Two methods implemented in `Machineshop` are *stacked regression* [@breiman:1996:SR] and *super learners* [@vanderLaan:2007:SL].  Stacked regression fits a linear combination of predictions from specified base learners to produce a prediction function of the form
$$
\hat{f}(x) = \sum_{m=1}^M \hat{w}_m \hat{f}_m(x).
$$
Stacking weights $w$ are estimated by (constrained) least squares regression of case responses $y_i$ on predictions $\hat{f}^{-\kappa(i)}(x_i)$ from learners fit to data subsamples $-\kappa(i)$ not containing the corresponding cases. In particular, they are obtained as the solution
$$
\hat{w} = \underset{w}{\operatorname{argmin}} \sum_{i=1}^{N}\left(y_i - \sum_{m=1}^{M} w_m \hat{f}^{-\kappa(i)}(x_i) \right)^2
$$
subject to the constraints that all $w_m \ge 0$ and $\sum_m w_m = 1$.  K-fold cross-validation is the default subsampling method employed in the estimation, with the other resampling methods provided by the package available as other options.  Survival outcomes are handled with a modified version of the stacked regression algorithm in which

* minimization of least squares is replaced by maximization of Harrell's concordance index [-@harrell:1982:EYM] to accommodate censoring, and
* prediction can only be performed on the same response type used for the model fit; i.e., either survival means or survival probabilities at set follow-up times.

Super learners are a generalization of stacked regression that fit a specified model, such as `GBMModel`, to case responses $y_i$, base learner predictions $\hat{f}^{-\kappa(i)}(x_i)$, and optionally also to the original predictor variables $x_i$.  Given below are examples of a stacked regression and super learner each fit with gradient boosted, random forest, and Cox regression base learners.  A separate gradient boosted model is used as the super learner in the latter.

```{r using_strategies_ensembles}
## Stacked regression
stackedmodel <- StackedModel(GLMBoostModel, CForestModel, CoxModel)
res_stacked <- resample(surv_fo, data = surv_train, model = stackedmodel)
summary(res_stacked)

## Super learner
supermodel <- SuperModel(GLMBoostModel, CForestModel, CoxModel,
                         model = GBMModel)
res_super <- resample(surv_fo, data = surv_train, model = supermodel)
summary(res_super)
```
